{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy-based Agents\n",
    "\n",
    "Re-write of code from [Simple Reinforcement Learning with Tensorflow: Part 2 - Policy-based Agents](https://medium.com/@awjuliani/super-simple-reinforcement-learning-tutorial-part-2-ded33892c724#.zh7rnjs25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T11:06:35.603303Z",
     "start_time": "2019-10-17T11:06:35.588303Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T11:06:36.109303Z",
     "start_time": "2019-10-17T11:06:36.084303Z"
    }
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulate environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T11:07:04.249303Z",
     "start_time": "2019-10-17T11:06:38.769303Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "env.reset()\n",
    "img = plt.imshow(env.render(mode='rgb_array')) # only call this once\n",
    "for _ in range(100):\n",
    "    img.set_data(env.render(mode='rgb_array')) # just update the data\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "    action = env.action_space.sample()\n",
    "    env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-16T09:37:43.331000Z",
     "start_time": "2019-10-16T09:37:43.313000Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward for this episode was: 13.0\n",
      "Reward for this episode was: 35.0\n",
      "Reward for this episode was: 13.0\n",
      "Reward for this episode was: 11.0\n",
      "Reward for this episode was: 15.0\n",
      "Reward for this episode was: 12.0\n",
      "Reward for this episode was: 129.0\n",
      "Reward for this episode was: 17.0\n",
      "Reward for this episode was: 13.0\n",
      "Reward for this episode was: 16.0\n"
     ]
    }
   ],
   "source": [
    "# Try running environment with random actions\n",
    "env.reset()\n",
    "reward_sum = 0\n",
    "num_games = 10\n",
    "num_game = 0\n",
    "while num_game < num_games:\n",
    "#     env.render()\n",
    "    observation, reward, done, _ = env.step(env.action_space.sample())\n",
    "    reward_sum += reward\n",
    "    if done:\n",
    "        print(\"Reward for this episode was: {}\".format(reward_sum))\n",
    "        reward_sum = 0\n",
    "        num_game += 1\n",
    "        env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up a Neural Network agent\n",
    "We will use a policy neural network that takes observations, passes them through a single hidden layer and then produces a probability of choosing a left/right movement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-16T09:39:47.432000Z",
     "start_time": "2019-10-16T09:39:37.504000Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Dev\\anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras.layers as layers\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "import keras.backend as K\n",
    "from keras.initializers import glorot_uniform\n",
    "\n",
    "def get_policy_model(env, hidden_layer_neurons, lr):\n",
    "    dimen = env.reset().shape\n",
    "    num_actions = env.action_space.n\n",
    "    inp = layers.Input(shape=dimen,name=\"input_x\")\n",
    "    adv = layers.Input(shape=[1], name=\"advantages\")\n",
    "    x = layers.Dense(hidden_layer_neurons, \n",
    "                     activation=\"relu\", \n",
    "                     use_bias=False,\n",
    "                     kernel_initializer=glorot_uniform(seed=42),\n",
    "                     name=\"dense_1\")(inp)\n",
    "    out = layers.Dense(num_actions, \n",
    "                       activation=\"softmax\", \n",
    "                       kernel_initializer=glorot_uniform(seed=42),\n",
    "                       use_bias=False,\n",
    "                       name=\"out\")(x)\n",
    "\n",
    "    def custom_loss(y_true, y_pred):\n",
    "        # actual: 0 predict: 0 -> log(0 * (0 - 0) + (1 - 0) * (0 + 0)) = -inf\n",
    "        # actual: 1 predict: 1 -> log(1 * (1 - 1) + (1 - 1) * (1 + 1)) = -inf\n",
    "        # actual: 1 predict: 0 -> log(1 * (1 - 0) + (1 - 1) * (1 + 0)) = 0\n",
    "        # actual: 0 predict: 1 -> log(0 * (0 - 1) + (1 - 0) * (0 + 1)) = 0\n",
    "        log_lik = K.log(y_true * (y_true - y_pred) + (1 - y_true) * (y_true + y_pred))\n",
    "        return K.mean(log_lik * adv, keepdims=True)\n",
    "        \n",
    "    model_train = Model(inputs=[inp, adv], outputs=out)\n",
    "    model_train.compile(loss=custom_loss, optimizer=Adam(lr))\n",
    "    model_predict = Model(inputs=[inp], outputs=out)\n",
    "    return model_train, model_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-16T09:40:32.841000Z",
     "start_time": "2019-10-16T09:40:32.838000Z"
    }
   },
   "outputs": [],
   "source": [
    "def discount_rewards(r, gamma=0.99):\n",
    "    \"\"\"Takes 1d float array of rewards and computes discounted reward\n",
    "    e.g. f([1, 1, 1], 0.99) -> [2.9701, 1.99, 1]\n",
    "    \"\"\"\n",
    "    prior = 0\n",
    "    out = []\n",
    "    for val in r:\n",
    "        new_val = val + prior * gamma\n",
    "        out.append(new_val)\n",
    "        prior = new_val\n",
    "    return np.array(out[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-16T09:40:53.454000Z",
     "start_time": "2019-10-16T09:40:53.450000Z"
    }
   },
   "outputs": [],
   "source": [
    "# Constants defining our neural network\n",
    "hidden_layer_neurons = 8\n",
    "gamma = .99\n",
    "dimen = len(env.reset())\n",
    "print_every = 100\n",
    "batch_size = 50\n",
    "num_episodes = 10000\n",
    "render = False\n",
    "lr = 1e-2\n",
    "goal = 190"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-16T09:43:29.386000Z",
     "start_time": "2019-10-16T09:43:29.379000Z"
    }
   },
   "outputs": [],
   "source": [
    "# See our trained bot in action\n",
    "def score_model(model, num_tests, render=False):\n",
    "    scores = []    \n",
    "    for num_test in range(num_tests):\n",
    "        observation = env.reset()\n",
    "        reward_sum = 0\n",
    "        while True:\n",
    "            if render:\n",
    "                env.render()\n",
    "\n",
    "            state = np.reshape(observation, [1, dimen])\n",
    "            predict = model.predict([state])[0]\n",
    "            action = np.argmax(predict)\n",
    "            observation, reward, done, _ = env.step(action)\n",
    "            reward_sum += reward\n",
    "            if done:\n",
    "                break\n",
    "        scores.append(reward_sum)\n",
    "    env.close()\n",
    "    return np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-16T09:43:34.126000Z",
     "start_time": "2019-10-16T09:43:33.991000Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_x (InputLayer)         (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 8)                 32        \n",
      "_________________________________________________________________\n",
      "out (Dense)                  (None, 2)                 16        \n",
      "=================================================================\n",
      "Total params: 48\n",
      "Trainable params: 48\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_train, model_predict = get_policy_model(env, hidden_layer_neurons, lr)\n",
    "model_predict.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "reward_sum = 0\n",
    "\n",
    "num_actions = env.action_space.n\n",
    "\n",
    "# Placeholders for our observations, outputs and rewards\n",
    "states = np.empty(0).reshape(0,dimen)\n",
    "actions = np.empty(0).reshape(0,1)\n",
    "rewards = np.empty(0).reshape(0,1)\n",
    "discounted_rewards = np.empty(0).reshape(0,1)\n",
    "\n",
    "# Setting up our environment\n",
    "observation = env.reset()\n",
    "\n",
    "num_episode = 0\n",
    "\n",
    "losses = []\n",
    "\n",
    "while num_episode < num_episodes:\n",
    "    # Append the observations to our batch\n",
    "    state = np.reshape(observation, [1, dimen])\n",
    "    \n",
    "    predict = model_predict.predict([state])[0]\n",
    "    action = np.random.choice(range(num_actions),p=predict)\n",
    "    \n",
    "    # Append the observations and outputs for learning\n",
    "    states = np.vstack([states, state])\n",
    "    actions = np.vstack([actions, action])\n",
    "    \n",
    "    # Determine the oucome of our action\n",
    "    observation, reward, done, _ = env.step(action)\n",
    "    reward_sum += reward\n",
    "    rewards = np.vstack([rewards, reward])\n",
    "    \n",
    "    if done:\n",
    "        # Determine standardized rewards\n",
    "        discounted_rewards_episode = discount_rewards(rewards, gamma)       \n",
    "        discounted_rewards = np.vstack([discounted_rewards, discounted_rewards_episode])\n",
    "        \n",
    "        rewards = np.empty(0).reshape(0,1)\n",
    "\n",
    "        if (num_episode + 1) % batch_size == 0:\n",
    "            discounted_rewards -= discounted_rewards.mean()\n",
    "            discounted_rewards /= discounted_rewards.std()\n",
    "            discounted_rewards = discounted_rewards.squeeze()\n",
    "            actions = actions.squeeze().astype(int)\n",
    "           \n",
    "            actions_train = np.zeros([len(actions), num_actions])\n",
    "            actions_train[np.arange(len(actions)), actions] = 1\n",
    "            \n",
    "            loss = model_train.train_on_batch([states, discounted_rewards], actions_train)\n",
    "            losses.append(loss)\n",
    "\n",
    "            # Clear out game variables\n",
    "            states = np.empty(0).reshape(0,dimen)\n",
    "            actions = np.empty(0).reshape(0,1)\n",
    "            discounted_rewards = np.empty(0).reshape(0,1)\n",
    "\n",
    "\n",
    "        # Print periodically\n",
    "        if (num_episode + 1) % print_every == 0:\n",
    "            # Print status\n",
    "            score = score_model(model_predict,10)\n",
    "            print(\"Average reward for training episode {}: {:0.2f} Test Score: {:0.2f} Loss: {:0.6f} \".format(\n",
    "                (num_episode + 1), reward_sum/print_every, \n",
    "                score,\n",
    "                np.mean(losses[-print_every:])))\n",
    "            \n",
    "            if score >= goal:\n",
    "                print(\"Solved in {} episodes!\".format(num_episode))\n",
    "                break\n",
    "            reward_sum = 0\n",
    "                \n",
    "        num_episode += 1\n",
    "        observation = env.reset()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
